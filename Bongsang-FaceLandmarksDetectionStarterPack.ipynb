{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Bongsang Kim\n",
    "- homepage: https://bongsang.github.io\n",
    "- Linkedin: https://www.linkedin.com/in/bongsang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face landmarks detection by CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Data Analysis\n",
    "\n",
    "The first step is to load in the images of faces and their keypoints and visualize them.\n",
    "This set of image data has been extracted from the [YouTube Faces Dataset](https://www.cs.tau.ac.il/~wolf/ytfaces/), which includes videos of people in YouTube videos. These videos have been fed through some processing steps and turned into sets of image frames containing one face and the associated keypoints.\n",
    "\n",
    "<img src=\"https://www.cs.tau.ac.il/~wolf/ytfaces/logo.jpg\" width=50%>\n",
    "\n",
    "#### Training and Testing Data\n",
    "\n",
    "This facial landmarks dataset consists of 5,770 images. All of these images are separated into either a training or a test set of data.\n",
    "\n",
    "Total | Training | Testing\n",
    "----- | -------- | -------\n",
    "5,770 | 3,462    | 2,308\n",
    "100%  | 60%      | 40%\n",
    "\n",
    "The information about the images and keypoints in this dataset are summarized in CSV files, which we can read in using `pandas`. Let's read the training CSV and get the annotations in an (N, 2) array where N is the number of keypoints and 2 is the dimension of the keypoint coordinates (x, y).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# import glob\n",
    "\n",
    "# import skimage.io as io\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms, utils\n",
    "\n",
    "# # Bongsang's personal pacakges\n",
    "# from landmarks_dataset import LandmarksDataset\n",
    "# from landmarks_transform import Rescale, RandomCrop, Normalize, ToTensor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "landmarks_frame = pd.read_csv('data/training_landmarks.csv')\n",
    "print(f'The number of training dataset is {len(landmarks_frame)}. {3462/5770*100}% of 5,770')\n",
    "landmarks_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting some images from data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_image_name(frame, idx):\n",
    "    return frame.iloc[idx, 0]\n",
    "\n",
    "def get_landmarks(frame, idx):\n",
    "    landmark_points = frame.iloc[idx, 1:].to_numpy()\n",
    "    landmark_points = landmark_points.astype('float').reshape(-1, 2)\n",
    "    return landmark_points\n",
    "\n",
    "def show_landmarks(image, points, name):\n",
    "    plt.title(name)\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(points[:, 0], points[:, 1], s=20, marker='.', c='m')\n",
    "    plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    idx = np.random.randint(0, len(landmarks_frame))\n",
    "    image_name = get_image_name(landmarks_frame, idx)\n",
    "    image_path = os.path.join('data/training/', image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "    image_data = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    landmarks = get_landmarks(landmarks_frame, idx)\n",
    "    show_landmarks(image_data, landmarks, image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class and Transformations\n",
    "\n",
    "To prepare our data for training, we'll be using PyTorch's Dataset class. Much of this this code is a modified version of what can be found in the [PyTorch data loading tutorial](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_images/landmarked_face2.png\" width=25%>\n",
    "\n",
    "### PyTorch Dataset class\n",
    "\n",
    "``torch.utils.data.Dataset`` is an abstract class representing a\n",
    "dataset. This class will allow us to load batches of image/keypoint data, and uniformly apply transformations to our data, such as rescaling and normalizing images for training a neural network.\n",
    "\n",
    "\n",
    "Your custom dataset should inherit ``Dataset`` and override the following\n",
    "methods:\n",
    "\n",
    "-  ``__len__`` so that ``len(dataset)`` returns the size of the dataset.\n",
    "-  ``__getitem__`` to support the indexing such that ``dataset[i]`` can\n",
    "   be used to get the i-th sample of image/keypoint data.\n",
    "\n",
    "Let's create a dataset class for our face keypoints dataset. We will\n",
    "read the CSV file in ``__init__`` but leave the reading of images to\n",
    "``__getitem__``. This is memory efficient because all the images are not\n",
    "stored in the memory at once but read as required.\n",
    "\n",
    "A sample of our dataset will be a dictionary\n",
    "``{'image': image, 'keypoints': key_pts}``. Our dataset will take an\n",
    "optional argument ``transform`` so that any required processing can be\n",
    "applied on the sample. We will see the usefulness of ``transform`` in the\n",
    "next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LandmarksDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_name = self.landmarks_frame.iloc[idx, 0]\n",
    "        image_path = os.path.join(self.root_dir, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "        image_data = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image_data, 'landmarks': landmarks, 'name': image_name}\n",
    "        print(f'LandmarksDataset shape, file={image_name}, image={image_data.shape}, landmarks={landmarks.shape}')\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting some images from PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "landmarks_dataset = LandmarksDataset(csv_file='data/training_landmarks.csv',\n",
    "                                    root_dir='data/training',\n",
    "                                    transform=None)\n",
    "for _ in range(2):\n",
    "    idx = np.random.randint(0, len(landmarks_dataset))\n",
    "    sample = landmarks_dataset[idx]\n",
    "    show_landmarks(sample['image'], sample['landmarks'], sample['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Transforms\n",
    "\n",
    "Now, the images above are not of the same size, and neural networks often expect images that are standardized; a fixed size, with a normalized range for color ranges and coordinates, and (for PyTorch) converted from numpy lists and arrays to Tensors.\n",
    "\n",
    "Therefore, we will need to write some pre-processing code.\n",
    "Let's create four transforms:\n",
    "\n",
    "-  ``Normalize``: to convert a color image to grayscale values with a range of [0,1] and normalize the keypoints to be in a range of about [-1, 1]\n",
    "-  ``Rescale``: to rescale an image to a desired size.\n",
    "-  ``RandomCrop``: to crop an image randomly.\n",
    "-  ``ToTensor``: to convert numpy images to torch images.\n",
    "\n",
    "\n",
    "We will write them as callable classes instead of simple functions so\n",
    "that parameters of the transform need not be passed everytime it's\n",
    "called. For this, we just need to implement ``__call__`` method and \n",
    "(if we require parameters to be passed in), the ``__init__`` method. \n",
    "We can then use a transform like this:\n",
    "\n",
    "    tx = Transform(params)\n",
    "    transformed_sample = tx(sample)\n",
    "\n",
    "Observe below how these transforms are generally applied to both the image and its keypoints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"Convert a color image to grayscale and normalize the color range to [0,1].\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks, name = sample['image'], sample['landmarks'], sample['name']\n",
    "\n",
    "\n",
    "        image_copy = np.copy(image)\n",
    "        landmarks_copy = np.copy(landmarks)\n",
    "\n",
    "        # convert image to grayscale\n",
    "        image_copy = cv2.cvtColor(image_copy, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # scale color range from [0, 255] to [0, 1]\n",
    "        image_copy = image_copy / 255.0\n",
    "\n",
    "        # scale landmarks to be centered around 0 with a range of [-1, 1]\n",
    "        # mean = 100, sqrt = 50, so, pts should be (pts - 100)/50\n",
    "        landmarks_copy = (landmarks_copy - 100) / 50.0\n",
    "\n",
    "        print(f'Transformed LandmarksDataset shape, file={name}, rescaled shape = {image_copy.shape}')\n",
    "        \n",
    "        return {'image':image_copy, 'landmarks':landmarks_copy, 'name': name}\n",
    "\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks, name = sample['image'], sample['landmarks'], sample['name']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        image_copy = cv2.resize(image, (new_w, new_h))\n",
    "        landmarks_copy = landmarks * [new_w / w, new_h / h]\n",
    "        \n",
    "        print(f'Rescaled shape, file={name}, rescaled shape = {image_copy.shape}')\n",
    "\n",
    "        return {'image':image_copy, 'landmarks':landmarks_copy, 'name': name}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks, name = sample['image'], sample['landmarks'], sample['name']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image_copy = image[top: top + new_h, left: left + new_w]\n",
    "        landmarks_copy = landmarks - [left, top]\n",
    "\n",
    "        print(f'Croped shape, file={name}, rescaled shape = {image_copy.shape}')\n",
    "\n",
    "        return {'image':image_copy, 'landmarks':landmarks_copy, 'name': name}\n",
    "\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks, name = sample['image'], sample['landmarks'], sample['name']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image_copy = image.transpose((2, 0, 1))\n",
    "        print(f'Transformed LandmarksDataset shape, file={name}, rescaled shape = {image_copy.shape}')\n",
    "\n",
    "        return {'image': torch.from_numpy(image_copy), 'landmarks': torch.from_numpy(landmarks), 'name': name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out the transforms: Rescale any images to 250 size (height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "rescale = Rescale(250)\n",
    "\n",
    "idx = np.random.randint(0, len(landmarks_dataset))\n",
    "sample = landmarks_dataset[idx]\n",
    "transformed_sample = rescale(sample)\n",
    "show_landmarks(transformed_sample['image'], transformed_sample['landmarks'], transformed_sample['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out the transforms: Crop any images to 50 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "crop = RandomCrop(80)\n",
    "\n",
    "idx = np.random.randint(0, len(landmarks_dataset))\n",
    "sample = landmarks_dataset[idx]\n",
    "transformed_sample = crop(sample)\n",
    "show_landmarks(transformed_sample['image'], transformed_sample['landmarks'], transformed_sample['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out the transforms: Rescale to 250 size and then crop 224 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "composed = transforms.Compose([Rescale(250), RandomCrop(224)])\n",
    "\n",
    "idx = np.random.randint(0, len(landmarks_dataset))\n",
    "sample = landmarks_dataset[idx]\n",
    "transformed_sample = composed(sample)\n",
    "show_landmarks(transformed_sample['image'], transformed_sample['landmarks'], transformed_sample['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the transformed dataset\n",
    "\n",
    "Apply the transforms in order to get grayscale images of the same shape. Verify that your transform works by printing out the shape of the resulting data (printing out a few examples should show you a consistent tensor size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# define the data tranform\n",
    "# order matters! i.e. rescaling should come before a smaller crop\n",
    "data_transform = transforms.Compose([Rescale(250),\n",
    "                                     RandomCrop(224),\n",
    "                                     Normalize(),\n",
    "                                     ToTensor()])\n",
    "\n",
    "# create the transformed dataset\n",
    "transformed_dataset = FacialKeypointsDataset(csv_file='data/training_frames_keypoints.csv',\n",
    "                                             root_dir='data/training/',\n",
    "                                             transform=data_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# print some stats about the transformed data\n",
    "print('Number of images: ', len(transformed_dataset))\n",
    "\n",
    "# make sure the sample tensors are the expected size\n",
    "for i in range(5):\n",
    "    sample = transformed_dataset[i]\n",
    "    print(i, sample['image'].size(), sample['keypoints'].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Iteration and Batching\n",
    "\n",
    "Right now, we are iterating over this data using a ``for`` loop, but we are missing out on a lot of PyTorch's dataset capabilities, specifically the abilities to:\n",
    "\n",
    "-  Batch the data\n",
    "-  Shuffle the data\n",
    "-  Load the data in parallel using ``multiprocessing`` workers.\n",
    "\n",
    "``torch.utils.data.DataLoader`` is an iterator which provides all these\n",
    "features, and we'll see this in use in the *next* notebook, Notebook 2, when we load data in batches to train a neural network!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to Train!\n",
    "\n",
    "Now that you've seen how to load and transform our data, you're ready to build a neural network to train on this data.\n",
    "\n",
    "In the next notebook, you'll be tasked with creating a CNN for facial keypoint detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
